{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22a89fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import gym\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from game.pricing_env import PricingGameEnv\n",
    "from data.generator import SimpleDemandGenerator\n",
    "from gym.wrappers import FlattenObservation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "926858e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mish(input):\n",
    "    return input * torch.tanh(F.softplus(input))\n",
    "\n",
    "class Mish(nn.Module):\n",
    "    def __init__(self): super().__init__()\n",
    "    def forward(self, input): return mish(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f5d482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to convert numpy arrays to tensors\n",
    "def t(x):\n",
    "    x = np.array(x) if not isinstance(x, np.ndarray) else x\n",
    "    return torch.from_numpy(x).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0eedfddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_std = 3.0\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, n_actions, activation=nn.Tanh, hidden_size = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.n_actions = n_actions\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            activation(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            activation(),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "        \n",
    "#         logstds_param = nn.Parameter(torch.full((n_actions,), log_std))\n",
    "#         self.register_parameter(\"logstds\", logstds_param)\n",
    "    \n",
    "#     def forward(self, X):\n",
    "#         means = torch.sigmoid(self.model(X)) * 70\n",
    "#         stds = torch.clamp(self.logstds.exp(), 3, 30)\n",
    "        \n",
    "#         return torch.distributions.Normal(means, stds)\n",
    "\n",
    "        logstds_param = nn.Parameter(torch.full((n_actions,), 0.1))\n",
    "        self.register_parameter(\"logstds\", logstds_param)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        means = self.model(X)\n",
    "        stds = torch.clamp(self.logstds.exp(), 1e-3, 50)\n",
    "        \n",
    "        return torch.distributions.Normal(means, stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c32ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "468fe254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def __init__(self, state_dim, hidden_dim, init_w=3e-3):\n",
    "#         super(ValueNetwork, self).__init__()\n",
    "        \n",
    "#         self.linear1 = nn.Linear(state_dim, hidden_dim)\n",
    "#         self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "#         self.linear3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "#         self.linear3.weight.data.uniform_(-init_w, init_w)\n",
    "#         self.linear3.bias.data.uniform_(-init_w, init_w)\n",
    "        \n",
    "#     def forward(self, state):\n",
    "#         x = F.relu(self.linear1(state))\n",
    "#         x = F.relu(self.linear2(x))\n",
    "#         x = self.linear3(x)\n",
    "#         return x\n",
    "    \n",
    "def init_weights(layer,  init_w=3e-3):\n",
    "    if type(layer) == nn.Linear:\n",
    "        nn.init.uniform_(layer.weight, -init_w, init_w)\n",
    "        nn.init.uniform_(layer.bias, -init_w, init_w)\n",
    "    \n",
    "\n",
    "## Critic module\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, activation=nn.ReLU, hidden_size = 512):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_size),\n",
    "            activation(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            activation(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "        self.model.apply(init_weights) \n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fc824ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discounted_rewards(rewards, dones, gamma):\n",
    "    ret = 0\n",
    "    discounted = []\n",
    "    for reward, done in zip(rewards[::-1], dones[::-1]):\n",
    "        ret = reward + ret * gamma * (1-done)\n",
    "        discounted.append(ret)\n",
    "    \n",
    "    return discounted[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e14984c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_memory(memory, gamma=0.99, discount_rewards=True):\n",
    "    actions = []\n",
    "    states = []\n",
    "    next_states = []\n",
    "    rewards = []\n",
    "    dones = []\n",
    "\n",
    "    for action, reward, state, next_state, done in memory:\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        states.append(state)\n",
    "        next_states.append(next_state)\n",
    "        dones.append(done)\n",
    "    \n",
    "    if discount_rewards:\n",
    "        if False and dones[-1] == 0:\n",
    "            rewards = discounted_rewards(rewards + [last_value], dones + [0], gamma)[:-1]\n",
    "        else:\n",
    "            rewards = discounted_rewards(rewards, dones, gamma)\n",
    "\n",
    "    actions = t(actions).view(-1, 1)\n",
    "    states = t(states)\n",
    "    next_states = t(next_states)\n",
    "    rewards = t(rewards).view(-1, 1)\n",
    "    dones = t(dones).view(-1, 1)\n",
    "    return actions, rewards, states, next_states, dones\n",
    "\n",
    "def clip_grad_norm_(module, max_grad_norm):\n",
    "    nn.utils.clip_grad_norm_([p for g in module.param_groups for p in g[\"params\"]], max_grad_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958388aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2CLearner():\n",
    "    def __init__(self, actor, critic, gamma=0.9, entropy_beta=0,\n",
    "                 actor_lr=4e-4, critic_lr=4e-3, max_grad_norm=0.5):\n",
    "        self.gamma = gamma\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "        self.entropy_beta = entropy_beta\n",
    "        self.actor_optim = torch.optim.Adam(actor.parameters(), lr=actor_lr)\n",
    "        self.critic_optim = torch.optim.Adam(critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def learn(self, memory, steps, discount_rewards=True):\n",
    "        actions, rewards, states, next_states, dones = process_memory(memory, self.gamma, discount_rewards)\n",
    "\n",
    "        if discount_rewards:\n",
    "            td_target = rewards\n",
    "        else:\n",
    "            td_target = rewards + self.gamma*critic(next_states)*(1-dones)\n",
    "        value = critic(states)\n",
    "        advantage = td_target - value\n",
    "        \n",
    "        # actor\n",
    "        norm_dists = self.actor(states)\n",
    "        actions = actions.reshape(states.shape[0], -1)\n",
    "\n",
    "        logs_probs = norm_dists.log_prob(actions)\n",
    "        entropy = norm_dists.entropy().mean()\n",
    "        \n",
    "        actor_loss = (-logs_probs*advantage.detach()).mean() - entropy*self.entropy_beta\n",
    "        self.actor_optim.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        \n",
    "        clip_grad_norm_(self.actor_optim, self.max_grad_norm)\n",
    "        writer.add_histogram(\"gradients/actor\",\n",
    "                             torch.cat([p.grad.view(-1) for p in self.actor.parameters()]), global_step=steps)\n",
    "        writer.add_histogram(\"parameters/actor\",\n",
    "                             torch.cat([p.data.view(-1) for p in self.actor.parameters()]), global_step=steps)\n",
    "        self.actor_optim.step()\n",
    "\n",
    "        # critic\n",
    "        critic_loss = F.mse_loss(td_target, value)\n",
    "        self.critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        clip_grad_norm_(self.critic_optim, self.max_grad_norm)\n",
    "        writer.add_histogram(\"gradients/critic\",\n",
    "                             torch.cat([p.grad.view(-1) for p in self.critic.parameters()]), global_step=steps)\n",
    "        writer.add_histogram(\"parameters/critic\",\n",
    "                             torch.cat([p.data.view(-1) for p in self.critic.parameters()]), global_step=steps)\n",
    "        self.critic_optim.step()\n",
    "        \n",
    "        # reports\n",
    "        writer.add_scalar(\"losses/log_probs\", -logs_probs.mean(), global_step=steps)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy, global_step=steps) \n",
    "        writer.add_scalar(\"losses/entropy_beta\", self.entropy_beta, global_step=steps) \n",
    "        writer.add_scalar(\"losses/actor\", actor_loss, global_step=steps)\n",
    "        writer.add_scalar(\"losses/advantage\", advantage.mean(), global_step=steps)\n",
    "        writer.add_scalar(\"losses/critic\", critic_loss, global_step=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "034671e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state = None\n",
    "        self.done = True\n",
    "        self.steps = 0\n",
    "        self.episode_reward = 0\n",
    "        self.episode_rewards = []\n",
    "    \n",
    "    def reset(self):\n",
    "        self.episode_reward = 0\n",
    "        self.done = False\n",
    "        self.state = self.env.reset()\n",
    "    \n",
    "    def run(self, max_steps, memory=None):\n",
    "        if not memory: memory = []\n",
    "        \n",
    "        for i in range(max_steps):\n",
    "            if self.done: self.reset()\n",
    "            dists = actor(t(self.state))\n",
    "            actions = dists.sample().detach().data.numpy()\n",
    "            actions_clipped = np.clip(actions, self.env.action_space.low.min(), env.action_space.high.max())\n",
    "\n",
    "            next_state, reward, self.done, info = self.env.step(actions_clipped)\n",
    "            memory.append((actions, reward, self.state, next_state, self.done))\n",
    "\n",
    "            self.state = next_state\n",
    "            self.steps += 1\n",
    "            self.episode_reward += reward\n",
    "            \n",
    "            if self.done:\n",
    "                self.episode_rewards.append(self.episode_reward)\n",
    "                if len(self.episode_rewards) % 50 == 0:\n",
    "                    print(\"episode:\", len(self.episode_rewards), \", episode reward:\", self.episode_reward)\n",
    "                writer.add_scalar(\"episode_reward\", self.episode_reward, global_step=self.steps)\n",
    "                    \n",
    "        \n",
    "        return memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9a389b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.generator import DemandGenerator\n",
    "from game.state import GameState\n",
    "from ast import Dict\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "class ReducedPricingGameEnv(PricingGameEnv):\n",
    "     def __init__(\n",
    "        self,\n",
    "        demand_generator: DemandGenerator,\n",
    "        num_products: int,\n",
    "        num_weeks: int = 52,\n",
    "        min_price: float = 10.0,\n",
    "        max_price: float = 200.0,\n",
    "        min_cogs: float = 0.6,\n",
    "        max_cogs: float = 0.9,\n",
    "        max_initial_stock: int = 2000,\n",
    "        profit_lack_penalty: float = 10.0,\n",
    "        target_profit_ratio: float = 0.05,\n",
    "    ):\n",
    "        super().__init__(demand_generator,  # DemandGenerator object\n",
    "            num_products,  # int\n",
    "            num_weeks,  # int\n",
    "            min_price,  # float\n",
    "            max_price,  # float\n",
    "            min_cogs,  # float\n",
    "            max_cogs,  # float\n",
    "            max_initial_stock,  # int\n",
    "            profit_lack_penalty,  # float\n",
    "            target_profit_ratio) \n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"cw\": spaces.Box(low=0, high=200, shape=(1,), dtype=np.int32),\n",
    "                \"sales\": spaces.Box(low=0, high=np.inf, shape=(self.num_products,), dtype=np.float32),\n",
    "                #\"black_prices\": spaces.Box(low=0, high=np.inf, shape=(self.num_products,), dtype=np.float32),\n",
    "                #\"residual_value\": spaces.Box(low=0, high=np.inf, shape=(self.num_products,), dtype=np.float32),\n",
    "#                 \"article_season_end\": spaces.Box(\n",
    "#                     low=0, high=self.max_initial_stock, shape=(self.num_products,), dtype=np.int32\n",
    "#                 ),\n",
    "                \"stocks\": spaces.Box(low=0, high=self.max_initial_stock, shape=(self.num_products,), dtype=np.int32),\n",
    "                \"revenues\": spaces.Box(low=0, high=np.inf, shape=(self.num_products,), dtype=np.float32),\n",
    "                #\"profits\": spaces.Box(low=-np.inf, high=np.inf, shape=(self.num_products,), dtype=np.float32),\n",
    "                \"discounts\": spaces.Box(low=0, high=100, shape=(self.num_products,), dtype=np.float32),\n",
    "                #\"online_status\": spaces.Box(low=0, high=1, shape=(self.num_products,), dtype=np.int32),\n",
    "\n",
    "\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        def _get_observation(self) -> Dict:\n",
    "            observations = {\n",
    "                \"cw\": self.current_cw,\n",
    "                \"sales\": self.sales[-1],\n",
    "                #\"black_prices\": self.black_prices,\n",
    "                #\"residual_value\": self.residual_value,\n",
    "                #\"article_season_end\": self.article_season_end,\n",
    "                \"stocks\": self.stocks[-1],\n",
    "                \"revenues\": self.revenues[-1],\n",
    "                #\"profits\": self.profits[-1],\n",
    "                \"discounts\": self.discounts[-1],\n",
    "                #\"online_status\": self.online_status[-1]\n",
    "            }\n",
    "\n",
    "            return observations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cbd4f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "seed = 12315\n",
    "generator = SimpleDemandGenerator()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9911f13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FlattenObservation(ReducedPricingGameEnv(generator, n))\n",
    "writer = SummaryWriter(\"runs/mish_activation\",purge_step=0)\n",
    "\n",
    "# config\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "actor = Actor(state_dim, n_actions,activation=Mish, hidden_size=128)\n",
    "critic = Critic(state_dim,activation=nn.ReLU, hidden_size=128)\n",
    "\n",
    "learner = A2CLearner(actor, critic,max_grad_norm=400,actor_lr=4e-3, critic_lr=4e-2, gamma=0.99)\n",
    "runner = Runner(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef962ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 50 , episode reward: -24433238.960680235\n",
      "episode: 100 , episode reward: -15943621.307845956\n",
      "episode: 150 , episode reward: -20389502.511745665\n",
      "episode: 200 , episode reward: -15904729.367346194\n",
      "episode: 250 , episode reward: -16466849.883225795\n",
      "episode: 300 , episode reward: -15327229.048799472\n",
      "episode: 350 , episode reward: -13868919.470234677\n",
      "episode: 400 , episode reward: -11857518.593678057\n",
      "episode: 450 , episode reward: -11968761.41412711\n",
      "episode: 500 , episode reward: -14597731.188335042\n",
      "episode: 550 , episode reward: -14359862.480465943\n",
      "episode: 600 , episode reward: -13693554.854540357\n",
      "episode: 650 , episode reward: -7504210.543608316\n",
      "episode: 700 , episode reward: -17247413.093813695\n",
      "episode: 750 , episode reward: -14392002.246941712\n",
      "episode: 800 , episode reward: -10631525.639631767\n",
      "episode: 850 , episode reward: -7956170.701868766\n",
      "episode: 900 , episode reward: -11144965.711485121\n",
      "episode: 950 , episode reward: -15490820.02579205\n",
      "episode: 1000 , episode reward: -10717261.464292414\n",
      "episode: 1050 , episode reward: -16735427.014846481\n",
      "episode: 1100 , episode reward: -10388519.689885639\n",
      "episode: 1150 , episode reward: -17784058.050203864\n",
      "episode: 1200 , episode reward: -6914643.137035182\n",
      "episode: 1250 , episode reward: -6579864.584862297\n",
      "episode: 1300 , episode reward: -3665673.2664103108\n",
      "episode: 1350 , episode reward: -17398349.998744942\n",
      "episode: 1400 , episode reward: -15349827.71585213\n",
      "episode: 1450 , episode reward: -11576980.078944437\n",
      "episode: 1500 , episode reward: -6426377.781436259\n",
      "episode: 1550 , episode reward: -9940985.275636112\n",
      "episode: 1600 , episode reward: -7932463.715266254\n",
      "episode: 1650 , episode reward: -9228046.548212515\n",
      "episode: 1700 , episode reward: -14048553.861555254\n",
      "episode: 1750 , episode reward: -8905936.886129107\n",
      "episode: 1800 , episode reward: -13733130.215564776\n",
      "episode: 1850 , episode reward: -12777666.437588401\n",
      "episode: 1900 , episode reward: -10755124.997674372\n",
      "episode: 1950 , episode reward: -8153054.5929741\n",
      "episode: 2000 , episode reward: -11599976.554011332\n",
      "episode: 2050 , episode reward: -8379672.037216375\n",
      "episode: 2100 , episode reward: -11684092.862831248\n",
      "episode: 2150 , episode reward: -6784491.849916583\n",
      "episode: 2200 , episode reward: -9257537.566622797\n",
      "episode: 2250 , episode reward: -8600308.7889039\n",
      "episode: 2300 , episode reward: -10319358.946555154\n",
      "episode: 2350 , episode reward: -4251571.0226073\n",
      "episode: 2400 , episode reward: -4429209.17399458\n",
      "episode: 2450 , episode reward: -4466773.063924591\n",
      "episode: 2500 , episode reward: -13420817.698202517\n",
      "episode: 2550 , episode reward: -5785435.890688349\n",
      "episode: 2600 , episode reward: -9627248.383170968\n",
      "episode: 2650 , episode reward: -5404987.332756042\n",
      "episode: 2700 , episode reward: -4739242.510140169\n",
      "episode: 2750 , episode reward: -7998484.74970834\n",
      "episode: 2800 , episode reward: -16089598.027981859\n",
      "episode: 2850 , episode reward: -12562935.853813244\n",
      "episode: 2900 , episode reward: -12088100.222206619\n",
      "episode: 2950 , episode reward: -13530497.470713047\n",
      "episode: 3000 , episode reward: -3728938.71153728\n",
      "episode: 3050 , episode reward: -5339181.810974684\n",
      "episode: 3100 , episode reward: -6915206.2090039365\n",
      "episode: 3150 , episode reward: -8178099.44887289\n",
      "episode: 3200 , episode reward: -4742242.273124557\n",
      "episode: 3250 , episode reward: -5841178.513304751\n",
      "episode: 3300 , episode reward: -9625383.441109959\n",
      "episode: 3350 , episode reward: -9373230.340865374\n",
      "episode: 3400 , episode reward: -5121760.662899992\n",
      "episode: 3450 , episode reward: -8812369.468463363\n",
      "episode: 3500 , episode reward: -5633547.728605434\n",
      "episode: 3550 , episode reward: -5291598.2107666675\n",
      "episode: 3600 , episode reward: -8175253.049320752\n",
      "episode: 3650 , episode reward: -10099133.55057181\n",
      "episode: 3700 , episode reward: -9810535.759673614\n",
      "episode: 3750 , episode reward: -6319571.883849519\n",
      "episode: 3800 , episode reward: -9206165.757697238\n",
      "episode: 3850 , episode reward: -15948329.36137748\n",
      "episode: 3900 , episode reward: -15329387.762273356\n",
      "episode: 3950 , episode reward: -8918400.735670593\n",
      "episode: 4000 , episode reward: -1091220.3823928013\n",
      "episode: 4050 , episode reward: -9136655.08719748\n",
      "episode: 4100 , episode reward: -5710004.524728628\n",
      "episode: 4150 , episode reward: -7243251.154454879\n",
      "episode: 4200 , episode reward: -5859075.812314595\n",
      "episode: 4250 , episode reward: -8100893.853690062\n",
      "episode: 4300 , episode reward: -3741249.3223452596\n",
      "episode: 4350 , episode reward: -4973629.683850226\n",
      "episode: 4400 , episode reward: -6770683.322685195\n",
      "episode: 4450 , episode reward: -7081652.757744001\n",
      "episode: 4500 , episode reward: -11617032.705257874\n",
      "episode: 4550 , episode reward: -10222903.788501251\n",
      "episode: 4600 , episode reward: -7868541.198314989\n",
      "episode: 4650 , episode reward: -9314682.698376577\n",
      "episode: 4700 , episode reward: -8792007.987168858\n",
      "episode: 4750 , episode reward: -8460983.522907004\n",
      "episode: 4800 , episode reward: -7725185.545945495\n",
      "episode: 4850 , episode reward: -10956600.124145642\n",
      "episode: 4900 , episode reward: -6770779.5375444405\n",
      "episode: 4950 , episode reward: -10095176.7783372\n",
      "episode: 5000 , episode reward: -8331099.629973276\n",
      "episode: 5050 , episode reward: -15735854.465000866\n",
      "episode: 5100 , episode reward: -12059051.569377042\n",
      "episode: 5150 , episode reward: -7798787.530216686\n",
      "episode: 5200 , episode reward: -9518659.998958146\n",
      "episode: 5250 , episode reward: -6774980.032061977\n",
      "episode: 5300 , episode reward: -5908178.458912544\n",
      "episode: 5350 , episode reward: -3740426.9514552075\n",
      "episode: 5400 , episode reward: -5327190.174820226\n",
      "episode: 5450 , episode reward: -8550620.426639128\n",
      "episode: 5500 , episode reward: -11028189.222229796\n",
      "episode: 5550 , episode reward: -12316620.496120473\n",
      "episode: 5600 , episode reward: -6226304.385563072\n",
      "episode: 5650 , episode reward: -3565808.8632140225\n",
      "episode: 5700 , episode reward: -7533283.929462597\n",
      "episode: 5750 , episode reward: -4262370.610486163\n",
      "episode: 5800 , episode reward: -9413911.409621557\n",
      "episode: 5850 , episode reward: -4646412.170527449\n",
      "episode: 5900 , episode reward: -9055797.899649184\n",
      "episode: 5950 , episode reward: -3467530.2846599906\n",
      "episode: 6000 , episode reward: -3607326.5282008243\n",
      "episode: 6050 , episode reward: -8012129.477093117\n",
      "episode: 6100 , episode reward: -7635281.066635322\n",
      "episode: 6150 , episode reward: -4216416.105064222\n",
      "episode: 6200 , episode reward: -7859809.888392249\n",
      "episode: 6250 , episode reward: -6485456.487397552\n",
      "episode: 6300 , episode reward: -13458730.924262423\n",
      "episode: 6350 , episode reward: -14536433.608774528\n",
      "episode: 6400 , episode reward: -7950087.416755572\n",
      "episode: 6450 , episode reward: -4844874.265889603\n",
      "episode: 6500 , episode reward: -2780410.3805114767\n",
      "episode: 6550 , episode reward: -4706152.0148329185\n",
      "episode: 6600 , episode reward: -3335331.110129286\n",
      "episode: 6650 , episode reward: -5712610.449981981\n",
      "episode: 6700 , episode reward: -7593280.746606005\n",
      "episode: 6750 , episode reward: -7785018.741630401\n",
      "episode: 6800 , episode reward: -9805690.545454793\n",
      "episode: 6850 , episode reward: -8752412.821283646\n",
      "episode: 6900 , episode reward: -16109489.603103207\n",
      "episode: 6950 , episode reward: -1103477.9824170405\n",
      "episode: 7000 , episode reward: -12350765.957022572\n",
      "episode: 7050 , episode reward: -6450896.819552249\n",
      "episode: 7100 , episode reward: -8977399.186041014\n",
      "episode: 7150 , episode reward: -8925776.581006113\n",
      "episode: 7200 , episode reward: -16730502.09075166\n",
      "episode: 7250 , episode reward: -4640119.909215702\n",
      "episode: 7300 , episode reward: -12260956.747677106\n",
      "episode: 7350 , episode reward: -1703508.3668266442\n",
      "episode: 7400 , episode reward: -6215846.937610501\n",
      "episode: 7450 , episode reward: -11229430.98299216\n",
      "episode: 7500 , episode reward: -4015583.076125143\n",
      "episode: 7550 , episode reward: -17766238.331106573\n",
      "episode: 7600 , episode reward: -10646450.102768974\n",
      "episode: 7650 , episode reward: -15637466.13796097\n",
      "episode: 7700 , episode reward: -6295161.295213332\n",
      "episode: 7750 , episode reward: -7605668.112107299\n",
      "episode: 7800 , episode reward: -10782660.527329214\n",
      "episode: 7850 , episode reward: -5212967.470823845\n",
      "episode: 7900 , episode reward: -5418494.421604205\n",
      "episode: 7950 , episode reward: -12313406.231010105\n",
      "episode: 8000 , episode reward: -9576732.698687416\n",
      "episode: 8050 , episode reward: -12184496.786931016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 8100 , episode reward: -9248023.9359695\n",
      "episode: 8150 , episode reward: -3496166.39072299\n",
      "episode: 8200 , episode reward: -9872031.268796831\n",
      "episode: 8250 , episode reward: -6929603.62350058\n",
      "episode: 8300 , episode reward: -13860732.356726613\n",
      "episode: 8350 , episode reward: -5022033.015546044\n",
      "episode: 8400 , episode reward: -8306800.783045085\n",
      "episode: 8450 , episode reward: -7665259.192755082\n",
      "episode: 8500 , episode reward: -7166881.201509306\n",
      "episode: 8550 , episode reward: -6047876.608419925\n",
      "episode: 8600 , episode reward: -14512177.322511178\n",
      "episode: 8650 , episode reward: -5173809.519209393\n",
      "episode: 8700 , episode reward: -5883569.844243121\n",
      "episode: 8750 , episode reward: -15701349.170889378\n",
      "episode: 8800 , episode reward: -10361670.425032979\n",
      "episode: 8850 , episode reward: -4306108.252008416\n",
      "episode: 8900 , episode reward: -10185382.884643227\n",
      "episode: 8950 , episode reward: -12453285.501542632\n",
      "episode: 9000 , episode reward: -9921918.574267808\n",
      "episode: 9050 , episode reward: -4168805.9507180164\n",
      "episode: 9100 , episode reward: -6823817.257219331\n",
      "episode: 9150 , episode reward: -1367085.9114061152\n",
      "episode: 9200 , episode reward: -2673066.629454633\n",
      "episode: 9250 , episode reward: -3283461.0121919545\n",
      "episode: 9300 , episode reward: -4004837.982207152\n",
      "episode: 9350 , episode reward: -3858792.471581368\n",
      "episode: 9400 , episode reward: -3848548.9095152654\n",
      "episode: 9450 , episode reward: -7754763.288810397\n",
      "episode: 9500 , episode reward: -5777205.9522640295\n",
      "episode: 9550 , episode reward: -5648497.971490028\n",
      "episode: 9600 , episode reward: -7854684.754179454\n",
      "episode: 9650 , episode reward: -5727848.663916251\n",
      "episode: 9700 , episode reward: -4236743.325974794\n",
      "episode: 9750 , episode reward: 1254007.0947958499\n",
      "episode: 9800 , episode reward: -2593400.208059499\n",
      "episode: 9850 , episode reward: -1415299.5491970633\n",
      "episode: 9900 , episode reward: -11653738.740371212\n",
      "episode: 9950 , episode reward: -1197784.1342308528\n",
      "episode: 10000 , episode reward: -1175489.303789624\n",
      "episode: 10050 , episode reward: -4735551.141157562\n",
      "episode: 10100 , episode reward: -250094.51336919074\n",
      "episode: 10150 , episode reward: 781692.8504784729\n",
      "episode: 10200 , episode reward: -3753893.1678013494\n",
      "episode: 10250 , episode reward: -5951744.374050739\n",
      "episode: 10300 , episode reward: -1348992.5644595448\n",
      "episode: 10350 , episode reward: -7058815.538755861\n",
      "episode: 10400 , episode reward: -7983468.054134487\n",
      "episode: 10450 , episode reward: 2420280.7577114813\n",
      "episode: 10500 , episode reward: -2459191.6957992474\n",
      "episode: 10550 , episode reward: -6120758.050783204\n",
      "episode: 10600 , episode reward: -3536939.3526251772\n",
      "episode: 10650 , episode reward: -4420532.754000529\n",
      "episode: 10700 , episode reward: -4160667.6997026363\n",
      "episode: 10750 , episode reward: -2692335.2845372246\n",
      "episode: 10800 , episode reward: -4654445.987514053\n",
      "episode: 10850 , episode reward: -737294.2218744375\n",
      "episode: 10900 , episode reward: -2924024.494939315\n",
      "episode: 10950 , episode reward: -9394331.880102172\n",
      "episode: 11000 , episode reward: -3027799.0259310445\n",
      "episode: 11050 , episode reward: 2099133.4827513574\n",
      "episode: 11100 , episode reward: -3353990.442351935\n",
      "episode: 11150 , episode reward: -5905990.346032439\n",
      "episode: 11200 , episode reward: -4874993.53926417\n",
      "episode: 11250 , episode reward: 2239310.964572937\n",
      "episode: 11300 , episode reward: -6455997.084860224\n",
      "episode: 11350 , episode reward: -4339862.442819756\n",
      "episode: 11400 , episode reward: -1031545.2783055524\n",
      "episode: 11450 , episode reward: -995492.4970349537\n",
      "episode: 11500 , episode reward: 520895.6909682837\n",
      "episode: 11550 , episode reward: 243427.8528185694\n",
      "episode: 11600 , episode reward: -6440242.620055212\n",
      "episode: 11650 , episode reward: -4040694.2695401944\n",
      "episode: 11700 , episode reward: -6178121.880142866\n",
      "episode: 11750 , episode reward: -5246318.450959385\n",
      "episode: 11800 , episode reward: -2293682.877728197\n",
      "episode: 11850 , episode reward: -140769.13626655634\n",
      "episode: 11900 , episode reward: -4232808.659454005\n",
      "episode: 11950 , episode reward: -7155377.498931019\n",
      "episode: 12000 , episode reward: -11007782.032395575\n",
      "episode: 12050 , episode reward: 1175503.1132469184\n",
      "episode: 12100 , episode reward: 1256077.8609981406\n",
      "episode: 12150 , episode reward: 3613028.6462921984\n",
      "episode: 12200 , episode reward: -679689.6874238751\n",
      "episode: 12250 , episode reward: -2794460.5243233163\n",
      "episode: 12300 , episode reward: 2502689.444529198\n",
      "episode: 12350 , episode reward: -1513178.3472159964\n",
      "episode: 12400 , episode reward: 981036.41020733\n",
      "episode: 12450 , episode reward: -5406379.022506058\n",
      "episode: 12500 , episode reward: -5815647.606624963\n",
      "episode: 12550 , episode reward: 2291794.851298789\n",
      "episode: 12600 , episode reward: -2844485.286424872\n",
      "episode: 12650 , episode reward: -2767893.048928376\n",
      "episode: 12700 , episode reward: -4600627.161815105\n",
      "episode: 12750 , episode reward: -3932644.863461847\n",
      "episode: 12800 , episode reward: -6278959.061016709\n",
      "episode: 12850 , episode reward: -2435905.382586748\n",
      "episode: 12900 , episode reward: -8204132.125139072\n",
      "episode: 12950 , episode reward: -1764738.0388324533\n",
      "episode: 13000 , episode reward: -1495325.798926654\n",
      "episode: 13050 , episode reward: -2499462.119556105\n",
      "episode: 13100 , episode reward: -2817313.861282017\n",
      "episode: 13150 , episode reward: 2211406.1445295718\n",
      "episode: 13200 , episode reward: -3600722.108122519\n",
      "episode: 13250 , episode reward: -1961006.1434453386\n",
      "episode: 13300 , episode reward: -1554707.649159346\n",
      "episode: 13350 , episode reward: -373179.16243744036\n",
      "episode: 13400 , episode reward: -5236739.364999602\n",
      "episode: 13450 , episode reward: -9069208.338792913\n",
      "episode: 13500 , episode reward: -5810114.149068246\n",
      "episode: 13550 , episode reward: -5239769.333770087\n",
      "episode: 13600 , episode reward: -4145629.1571083004\n",
      "episode: 13650 , episode reward: -5635095.10534278\n",
      "episode: 13700 , episode reward: -861897.9876118551\n",
      "episode: 13750 , episode reward: -5078492.417467713\n",
      "episode: 13800 , episode reward: -998895.773994308\n",
      "episode: 13850 , episode reward: -3663075.869937436\n",
      "episode: 13900 , episode reward: -2945644.8903933885\n",
      "episode: 13950 , episode reward: -2875846.0158052538\n",
      "episode: 14000 , episode reward: 233349.07457265793\n",
      "episode: 14050 , episode reward: -3614824.9424067615\n",
      "episode: 14100 , episode reward: -5635101.718190136\n",
      "episode: 14150 , episode reward: -4026561.460253019\n",
      "episode: 14200 , episode reward: -2357593.923189895\n",
      "episode: 14250 , episode reward: -2468558.853903883\n",
      "episode: 14300 , episode reward: -6443945.671735278\n",
      "episode: 14350 , episode reward: -3441098.517627401\n",
      "episode: 14400 , episode reward: -281780.5827503612\n",
      "episode: 14450 , episode reward: -6073662.428918792\n",
      "episode: 14500 , episode reward: -4247346.75633619\n",
      "episode: 14550 , episode reward: -2734123.0508607267\n",
      "episode: 14600 , episode reward: -8709386.167140625\n",
      "episode: 14650 , episode reward: -7452078.922172803\n",
      "episode: 14700 , episode reward: -3812765.4872008106\n",
      "episode: 14750 , episode reward: -2112877.7405853216\n",
      "episode: 14800 , episode reward: -6495501.356563706\n",
      "episode: 14850 , episode reward: 3866870.2372769536\n",
      "episode: 14900 , episode reward: -6514034.40864954\n",
      "episode: 14950 , episode reward: -7326295.476459773\n",
      "episode: 15000 , episode reward: -10821432.547411587\n",
      "episode: 15050 , episode reward: -9096578.58881757\n",
      "episode: 15100 , episode reward: -747674.4899817444\n",
      "episode: 15150 , episode reward: -5331735.68084423\n",
      "episode: 15200 , episode reward: -6687934.680210058\n",
      "episode: 15250 , episode reward: 2440491.301431519\n",
      "episode: 15300 , episode reward: 2038924.3362437228\n",
      "episode: 15350 , episode reward: -3145844.3049538163\n",
      "episode: 15400 , episode reward: -2324049.089593884\n",
      "episode: 15450 , episode reward: -7581031.791135987\n",
      "episode: 15500 , episode reward: -3522607.5128516555\n",
      "episode: 15550 , episode reward: -4413720.074562005\n",
      "episode: 15600 , episode reward: 2982349.7706495617\n",
      "episode: 15650 , episode reward: -2474900.8281373954\n",
      "episode: 15700 , episode reward: -8089692.7482501045\n",
      "episode: 15750 , episode reward: -2363856.331043564\n",
      "episode: 15800 , episode reward: -366075.14594178787\n",
      "episode: 15850 , episode reward: 4030714.6738914168\n",
      "episode: 15900 , episode reward: -2743757.433872789\n",
      "episode: 15950 , episode reward: -2366642.313414201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 16000 , episode reward: -9656725.744529953\n",
      "episode: 16050 , episode reward: -1968849.0674479078\n",
      "episode: 16100 , episode reward: -6228282.3971499195\n",
      "episode: 16150 , episode reward: -943913.618873114\n",
      "episode: 16200 , episode reward: -3716435.726165846\n",
      "episode: 16250 , episode reward: -5274298.721924506\n",
      "episode: 16300 , episode reward: 78094.82387590385\n",
      "episode: 16350 , episode reward: -6955311.413567347\n",
      "episode: 16400 , episode reward: -7096286.263290383\n",
      "episode: 16450 , episode reward: -1965305.4826220386\n",
      "episode: 16500 , episode reward: -5178971.690951781\n",
      "episode: 16550 , episode reward: -4403447.452091985\n",
      "episode: 16600 , episode reward: -10052553.197239986\n",
      "episode: 16650 , episode reward: -7856922.384434719\n",
      "episode: 16700 , episode reward: -7068648.290231143\n",
      "episode: 16750 , episode reward: -5184537.605912433\n",
      "episode: 16800 , episode reward: -8194325.148238135\n",
      "episode: 16850 , episode reward: -3856750.4765754207\n",
      "episode: 16900 , episode reward: -3948430.6855367813\n",
      "episode: 16950 , episode reward: -1145646.9934873362\n",
      "episode: 17000 , episode reward: 3273868.9376018955\n",
      "episode: 17050 , episode reward: -2900838.848751856\n",
      "episode: 17100 , episode reward: 1590181.3669184495\n",
      "episode: 17150 , episode reward: 1495761.2224364588\n",
      "episode: 17200 , episode reward: 3102724.6667525107\n",
      "episode: 17250 , episode reward: -9106166.190855442\n",
      "episode: 17300 , episode reward: -1164034.023015265\n",
      "episode: 17350 , episode reward: -9036714.809365768\n",
      "episode: 17400 , episode reward: -2799030.2990629273\n",
      "episode: 17450 , episode reward: -8055172.23886901\n",
      "episode: 17500 , episode reward: 1074298.6542106925\n",
      "episode: 17550 , episode reward: -3781619.191717452\n",
      "episode: 17600 , episode reward: -1684319.970069599\n",
      "episode: 17650 , episode reward: -2142330.1001226157\n",
      "episode: 17700 , episode reward: -4311256.727308543\n",
      "episode: 17750 , episode reward: -2026783.0530693338\n",
      "episode: 17800 , episode reward: -7345487.374564366\n",
      "episode: 17850 , episode reward: 2986727.6642198423\n",
      "episode: 17900 , episode reward: -9288717.580646629\n",
      "episode: 17950 , episode reward: -3390547.9169885656\n",
      "episode: 18000 , episode reward: 882216.3445113939\n",
      "episode: 18050 , episode reward: 3963219.964342422\n",
      "episode: 18100 , episode reward: -213181.73587242537\n",
      "episode: 18150 , episode reward: 797110.2058817311\n",
      "episode: 18200 , episode reward: -10586613.037730921\n",
      "episode: 18250 , episode reward: -1346562.1424727018\n",
      "episode: 18300 , episode reward: 2901055.65761347\n",
      "episode: 18350 , episode reward: -239712.05202719546\n",
      "episode: 18400 , episode reward: -1341896.2971837043\n",
      "episode: 18450 , episode reward: -5688661.120063017\n",
      "episode: 18500 , episode reward: -3601058.2834039116\n",
      "episode: 18550 , episode reward: -13135860.396010734\n",
      "episode: 18600 , episode reward: -2542555.8120379974\n",
      "episode: 18650 , episode reward: -5328274.010690537\n",
      "episode: 18700 , episode reward: -3835869.9465750037\n",
      "episode: 18750 , episode reward: -10770730.655647872\n",
      "episode: 18800 , episode reward: -5181395.477370207\n",
      "episode: 18850 , episode reward: -260112.02771753538\n",
      "episode: 18900 , episode reward: -4598303.410944581\n",
      "episode: 18950 , episode reward: -9907330.900624178\n",
      "episode: 19000 , episode reward: -3241230.7848329656\n",
      "episode: 19050 , episode reward: -1471853.372982519\n",
      "episode: 19100 , episode reward: -8289604.370761579\n",
      "episode: 19150 , episode reward: -5690395.762383867\n",
      "episode: 19200 , episode reward: -2231875.4030279946\n",
      "episode: 19250 , episode reward: -6112656.135186786\n",
      "episode: 19300 , episode reward: -2879109.174092833\n",
      "episode: 19350 , episode reward: -1327382.542213256\n",
      "episode: 19400 , episode reward: -7728324.101663115\n",
      "episode: 19450 , episode reward: -5896406.271038363\n",
      "episode: 19500 , episode reward: 2503818.7365163723\n",
      "episode: 19550 , episode reward: -6928355.789430253\n",
      "episode: 19600 , episode reward: -1835850.9071629061\n",
      "episode: 19650 , episode reward: 1568824.3500795865\n",
      "episode: 19700 , episode reward: 3074890.7727559805\n",
      "episode: 19750 , episode reward: 1330335.983416092\n",
      "episode: 19800 , episode reward: -7449830.069403252\n",
      "episode: 19850 , episode reward: -1337344.8294463926\n",
      "episode: 19900 , episode reward: 3108428.0629495\n",
      "episode: 19950 , episode reward: -2361021.0445526307\n",
      "episode: 20000 , episode reward: 2927858.604208879\n",
      "episode: 20050 , episode reward: 3949705.4244307764\n",
      "episode: 20100 , episode reward: -6322705.126051171\n",
      "episode: 20150 , episode reward: -406755.37870168034\n",
      "episode: 20200 , episode reward: -1477055.6701388324\n",
      "episode: 20250 , episode reward: -66506.49861018732\n",
      "episode: 20300 , episode reward: -1816361.5249129569\n",
      "episode: 20350 , episode reward: -2833803.426323958\n",
      "episode: 20400 , episode reward: -4444155.516781536\n",
      "episode: 20450 , episode reward: 3371962.1876908303\n",
      "episode: 20500 , episode reward: 2721980.1191567504\n",
      "episode: 20550 , episode reward: -6753655.320800447\n",
      "episode: 20600 , episode reward: -570217.6733539863\n",
      "episode: 20650 , episode reward: 2202868.1480866014\n",
      "episode: 20700 , episode reward: 2412130.1703730873\n",
      "episode: 20750 , episode reward: 2495604.464164937\n",
      "episode: 20800 , episode reward: -3910764.905103965\n",
      "episode: 20850 , episode reward: -997833.0755435138\n",
      "episode: 20900 , episode reward: -875503.1848183582\n",
      "episode: 20950 , episode reward: 4283759.153956253\n",
      "episode: 21000 , episode reward: -3989781.3746472606\n",
      "episode: 21050 , episode reward: -5492762.670548174\n",
      "episode: 21100 , episode reward: -8812840.877759933\n",
      "episode: 21150 , episode reward: -1972442.726847978\n",
      "episode: 21200 , episode reward: -370282.9568182421\n",
      "episode: 21250 , episode reward: -2231254.9385989383\n",
      "episode: 21300 , episode reward: 3197741.1310086027\n",
      "episode: 21350 , episode reward: 135227.0427502729\n",
      "episode: 21400 , episode reward: -6606723.440776354\n",
      "episode: 21450 , episode reward: 1005758.4182019164\n",
      "episode: 21500 , episode reward: -1294329.268762019\n",
      "episode: 21550 , episode reward: -3999603.244729872\n",
      "episode: 21600 , episode reward: -436126.2820773239\n",
      "episode: 21650 , episode reward: -1541538.3181971996\n",
      "episode: 21700 , episode reward: 967251.6196054204\n",
      "episode: 21750 , episode reward: 1479747.032747164\n",
      "episode: 21800 , episode reward: -4191272.1975167375\n",
      "episode: 21850 , episode reward: 819856.6168231848\n",
      "episode: 21900 , episode reward: 4962043.076179005\n",
      "episode: 21950 , episode reward: -1348741.4197406578\n",
      "episode: 22000 , episode reward: -8314135.596346881\n",
      "episode: 22050 , episode reward: 4238190.680798911\n",
      "episode: 22100 , episode reward: 3310858.0477280812\n",
      "episode: 22150 , episode reward: 470986.95661002654\n",
      "episode: 22200 , episode reward: -447559.53267818876\n",
      "episode: 22250 , episode reward: -2912668.6047554514\n",
      "episode: 22300 , episode reward: -3897257.596214756\n",
      "episode: 22350 , episode reward: 3458364.1752641154\n",
      "episode: 22400 , episode reward: 1772682.9635492938\n",
      "episode: 22450 , episode reward: -2018801.0087238536\n",
      "episode: 22500 , episode reward: -8589409.896220753\n",
      "episode: 22550 , episode reward: -4295056.81889287\n",
      "episode: 22600 , episode reward: 2504428.993986549\n",
      "episode: 22650 , episode reward: 2750732.417994831\n",
      "episode: 22700 , episode reward: -1061545.9933868006\n",
      "episode: 22750 , episode reward: 2505224.317709223\n",
      "episode: 22800 , episode reward: -3478030.964681364\n",
      "episode: 22850 , episode reward: -779240.4296040202\n",
      "episode: 22900 , episode reward: -1574180.4029674372\n",
      "episode: 22950 , episode reward: -4785432.762107711\n",
      "episode: 23000 , episode reward: 2540311.221982519\n",
      "episode: 23050 , episode reward: -734144.8172895666\n",
      "episode: 23100 , episode reward: -1426501.1238617338\n",
      "episode: 23150 , episode reward: 2859016.7677508397\n",
      "episode: 23200 , episode reward: -2236823.199181809\n",
      "episode: 23250 , episode reward: -1450006.9715329711\n",
      "episode: 23300 , episode reward: 5032465.958390687\n",
      "episode: 23350 , episode reward: -845048.6903782869\n",
      "episode: 23400 , episode reward: 1690873.5201902401\n",
      "episode: 23450 , episode reward: -345278.9786272459\n",
      "episode: 23500 , episode reward: -1115022.0918610333\n",
      "episode: 23550 , episode reward: -3197955.891257449\n",
      "episode: 23600 , episode reward: 1868519.0274908573\n",
      "episode: 23650 , episode reward: -752806.5739237079\n",
      "episode: 23700 , episode reward: -139403.15532936575\n",
      "episode: 23750 , episode reward: 1703395.2407412294\n",
      "episode: 23800 , episode reward: -175907.10621524812\n",
      "episode: 23850 , episode reward: 3535008.9815914957\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 23900 , episode reward: -3255762.643772308\n",
      "episode: 23950 , episode reward: 1003553.891619347\n",
      "episode: 24000 , episode reward: 1187399.029482032\n",
      "episode: 24050 , episode reward: 3766928.119181199\n",
      "episode: 24100 , episode reward: 4338247.82322317\n",
      "episode: 24150 , episode reward: 1761587.303926139\n",
      "episode: 24200 , episode reward: -5078421.675841009\n",
      "episode: 24250 , episode reward: -3520490.801459039\n",
      "episode: 24300 , episode reward: -3824053.03169349\n",
      "episode: 24350 , episode reward: 1925501.8438286695\n",
      "episode: 24400 , episode reward: -4320344.84312094\n",
      "episode: 24450 , episode reward: -9406747.85279297\n",
      "episode: 24500 , episode reward: -910578.1940548634\n",
      "episode: 24550 , episode reward: 3483073.478099292\n",
      "episode: 24600 , episode reward: -3191728.819245612\n",
      "episode: 24650 , episode reward: -676012.0317678486\n",
      "episode: 24700 , episode reward: 2632010.994993346\n",
      "episode: 24750 , episode reward: -5493498.991890058\n",
      "episode: 24800 , episode reward: -105745.50478827604\n",
      "episode: 24850 , episode reward: 394397.722313629\n",
      "episode: 24900 , episode reward: -156926.08280033222\n",
      "episode: 24950 , episode reward: 1232583.8606139256\n",
      "episode: 25000 , episode reward: 1363199.4144543244\n",
      "episode: 25050 , episode reward: -207330.32089813892\n",
      "episode: 25100 , episode reward: -8944026.2588984\n",
      "episode: 25150 , episode reward: -4185114.1015734356\n",
      "episode: 25200 , episode reward: -1755347.9439486589\n",
      "episode: 25250 , episode reward: 3026676.6640344216\n",
      "episode: 25300 , episode reward: -5955131.540636152\n",
      "episode: 25350 , episode reward: -477959.2179654641\n",
      "episode: 25400 , episode reward: -5956862.921069344\n",
      "episode: 25450 , episode reward: -6218674.347944339\n",
      "episode: 25500 , episode reward: -2338002.095321496\n",
      "episode: 25550 , episode reward: 4012166.0126465713\n",
      "episode: 25600 , episode reward: 2293489.771797258\n",
      "episode: 25650 , episode reward: -2973037.6635432653\n",
      "episode: 25700 , episode reward: -4070156.811754534\n",
      "episode: 25750 , episode reward: -5627912.298561067\n",
      "episode: 25800 , episode reward: 3235020.193873387\n",
      "episode: 25850 , episode reward: -3587741.0730557134\n",
      "episode: 25900 , episode reward: -7031706.174927674\n",
      "episode: 25950 , episode reward: -5947164.978857303\n",
      "episode: 26000 , episode reward: -396091.52553156205\n",
      "episode: 26050 , episode reward: 1151672.289199467\n",
      "episode: 26100 , episode reward: -4411391.219875976\n",
      "episode: 26150 , episode reward: -2554215.7830213183\n",
      "episode: 26200 , episode reward: -3398110.4422310414\n",
      "episode: 26250 , episode reward: 1908537.2643482767\n",
      "episode: 26300 , episode reward: 4283741.78599977\n",
      "episode: 26350 , episode reward: 388244.92443414684\n",
      "episode: 26400 , episode reward: 3671475.268147272\n",
      "episode: 26450 , episode reward: 2968978.4087200337\n",
      "episode: 26500 , episode reward: -247817.6321539469\n",
      "episode: 26550 , episode reward: 1030621.0318172288\n",
      "episode: 26600 , episode reward: -1771626.791606238\n",
      "episode: 26650 , episode reward: -8935123.629984556\n",
      "episode: 26700 , episode reward: 3054090.980340798\n",
      "episode: 26750 , episode reward: 964459.9525979774\n",
      "episode: 26800 , episode reward: -3433283.285114322\n",
      "episode: 26850 , episode reward: -1665327.8198179465\n",
      "episode: 26900 , episode reward: -886248.2315746022\n",
      "episode: 26950 , episode reward: -3176365.2551268428\n",
      "episode: 27000 , episode reward: -6321920.221023133\n",
      "episode: 27050 , episode reward: -5080211.818267979\n",
      "episode: 27100 , episode reward: -4301728.774942607\n",
      "episode: 27150 , episode reward: 1621297.7904092304\n",
      "episode: 27200 , episode reward: -7001280.897119222\n",
      "episode: 27250 , episode reward: -3930640.847171723\n",
      "episode: 27300 , episode reward: -3761211.6966095865\n",
      "episode: 27350 , episode reward: 196786.0844690695\n"
     ]
    }
   ],
   "source": [
    "steps_on_memory = 8000\n",
    "episodes = 100000\n",
    "episode_length = 53\n",
    "total_steps = (episode_length*episodes)//steps_on_memory\n",
    "\n",
    "for i in range(total_steps):\n",
    "    memory = runner.run(steps_on_memory)\n",
    "    learner.learn(memory, runner.steps, discount_rewards=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22fdefb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "seed = 12315\n",
    "state = None\n",
    "reward = 0\n",
    "done = False\n",
    "state = env.reset(seed=seed)\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    dists = actor(t(state))\n",
    "    value = critic(t(state))\n",
    "    actions = dists.sample().detach().data.numpy()\n",
    "    actions_clipped = np.clip(actions, env.action_space.low.min(), env.action_space.high.max())\n",
    "    print(actions, actions_clipped)\n",
    "    \n",
    "    state, reward, done, info = env.step(actions_clipped)\n",
    "    print(f\"Value {value}, reward {reward}\")\n",
    "\n",
    "    total_reward+=reward\n",
    "print(total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df792bbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.render(extended=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c648034a",
   "metadata": {},
   "outputs": [],
   "source": [
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcaf331",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5d6e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"Pendulum-v1\")\n",
    "writer = SummaryWriter(\"runs/mish_activation\",purge_step=0)\n",
    "\n",
    "# config\n",
    "state_dim = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.shape[0]\n",
    "actor = Actor(state_dim, n_actions, activation=Mish, hidden_size = 64)\n",
    "critic = Critic(state_dim, activation=Mish, hidden_size = 64)\n",
    "\n",
    "learner = A2CLearner(actor, critic)\n",
    "runner = Runner(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47360d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_on_memory = 16\n",
    "episodes = 800\n",
    "episode_length = 200\n",
    "total_steps = (episode_length*episodes)//steps_on_memory\n",
    "\n",
    "for i in range(total_steps):\n",
    "    memory = runner.run(steps_on_memory)\n",
    "    learner.learn(memory, runner.steps, discount_rewards=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1f6cf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
